# Configuración del servicio RAG
# Proveedor de LLM: "openai" o "gemini" (por defecto: gemini)
LLM_PROVIDER=gemini
LLM_TEMPERATURE=0.3

# OpenAI (solo si LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# Google Gemini (solo si LLM_PROVIDER=gemini)
GEMINI_API_KEY=your_gemini_api_key_here
# Modelos disponibles: gemini-1.5-pro-latest (recomendado para respuestas largas), 
# gemini-1.5-flash-latest (más rápido), gemini-2.0-flash-exp (experimental)
GEMINI_MODEL=gemini-1.5-pro-latest
# Tokens máximos de salida (8192 es el máximo para Gemini 1.5 Pro)
GEMINI_MAX_OUTPUT_TOKENS=8192
GEMINI_MAX_OUTPUT_TOKENS_BUDGET=8192
GEMINI_MAX_OUTPUT_TOKENS_PLAN=8192

# Embeddings
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
CHROMA_DB_PATH=./chroma_db

# Configuración del backend .NET
BACKEND_API_URL=http://localhost:5000
BACKEND_API_KEY=your_backend_api_key_here

# Configuración de archivos
UPLOAD_DIR=./uploads
GENERATED_BUDGETS_DIR=./generated_budgets
MAX_FILE_SIZE_MB=50

# Configuración de la aplicación
DEBUG=True
LOG_LEVEL=INFO
