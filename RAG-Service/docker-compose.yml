services:
  rag-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-service
    ports:
      - "8001:8001"
    environment:
      # Configuración del LLM
      - LLM_PROVIDER=gemini
      - LLM_TEMPERATURE=0.3
      
      # Gemini API (reemplazar con tu API key)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GEMINI_MODEL=gemini-1.5-flash-latest
      
      # OpenAI API (opcional, si usas OpenAI)
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - OPENAI_MODEL=gpt-4o-mini
      
      # Embeddings
      - EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
      - CHROMA_DB_PATH=./chroma_db
      
      # Backend .NET (ajustar según tu configuración)
      - BACKEND_API_URL=${BACKEND_API_URL:-http://host.docker.internal:5000}
      - BACKEND_API_KEY=${BACKEND_API_KEY}
      
      # Configuración de archivos
      - UPLOAD_DIR=./uploads
      - GENERATED_BUDGETS_DIR=./generated_budgets
      - MAX_FILE_SIZE_MB=50
      
      # Configuración de la aplicación
      - DEBUG=False
      - LOG_LEVEL=INFO
    
    volumes:
      # Persistencia de datos
      - ./chroma_db:/app/chroma_db
      - ./uploads:/app/uploads
      - ./generated_budgets:/app/generated_budgets
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge
